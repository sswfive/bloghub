---
title: 机器学习的流程
date: 2023-08-16 08:23:41
permalink: /pages/1cae68/
categories:
  - 机器学习
tags:
  - ml
simplePostListLength: 20
---
## 机器学习的背景
在机器学习领域有两大学派：频率学派和贝叶斯学派，而概率又是机器学习的核心角色，正是因为这两个学派的对概率的认知不同，于是产生了统计机器学习和概率图模型（符号学习）：

- 统计机器学习（基于数据）：
   - 核心是数据，利用不同的模型去拟合数据背后的规律，并用拟合出来的规律去判断和预测未知的结果。
   - 统计机器学习中最基础的模型就是线性回归，几乎所有其他模型都是从不同的角度对线性回归模型做出扩展和修正。
- 概率图模型（基于关系）：
   - 和基于数据的统计学习相比，基于关系的图模型更多的代表了因果推理的发展方向。
   - 贝叶斯注意到也需要计算待学习对象的概率分布，但它利用的不是海量的具体数据，而是变量之间的相互关系，每个变量的先验分布和大量复杂的积分技巧。
   - 图模型的学习方案是从 表示、推断、学习 三大方向进行开展。

统计机器学习是频率学派的核心观点，其解决问题的思路是：参数是确定的，数据是随机的，利用随机的数据推断确定的参数，得到的结果也是随机的；

:::tip

- 概率表示的是事件发生频率的极限值，即当重复试验的次数趋近于无穷大时，事件发生的频率会收敛到真实的概率之上（这种观点的前提是：概率是一个确定的值，并不会受到单次观察结果的影响）
- 频率统计理论的核心在于认定待估计的参数是固定不变的常量，讨论参数的概率分布是没有意义的；而用来估计参数的数据是随机的变量，每个数据都是参数支配下一次独立重复试验的结果。由于参数本身是确定的，那频率的波动就并非来源于参数本身的不确定性，而是由有限次观察造成的干扰而导致
:::
频率学派认为概率是随机事件发生频率的极限值，在执行参数估计时，**视参数为确定取值，视数据为随机变量**，主要采用 **最大似然估计法，让数据在给定参数下的似然概率最大化，**
**频率学派对应机器学习中的统计学习，以经验风险最小化作为模型选择的准则。**
## 机器学习的概述

- 定义：
   - 机器学习的定义：机器学习是一门研究通过计算的手段利用经验来改善系统自身性能的学科（源自：机器学习领域元老-汤姆·米切尔(Tom M. Mitchell)之口）
   - 统计机器学习的定义：通过对给定的指标（比如似然函数或者均方误差）进行最优化，来估计模型中参数的取值，估计是并不考虑参数的不确定性，也就是不考虑未知参数的先验分布。**和参数相关的信息全部来源于数据，输出的则是未知参数唯一的估计结果，这是统计机器学习的核心特征**
- 机器学习的任务：
   - 基于对已知数据构造概率模型，再运用概率模型对未知数据进行预测与分析
- 统计机器学习的精度如何衡量？
   - 引入经验风险来解决，用训练数据的经验分布替换掉原始表达式中数据的真实分布，借此将风险函数转化成可计算的数值（**损失函数（loss funciton**）直接定义了模型性能的度量方式，其数学期望被称为风险（risk）,风险最小化就是参数估计的依据和准测
   - 所谓的最优模型也就是使经验风险最小化（empirical risk minimization）的那个模型

## 统计机器学习核心
> 下文关于机器学习的表达均指统计机器学习，简称：机器学习

从历史数据中自动分析获取模型，并利用模型对未知数据进行预测![](https://cdn.staticaly.com/gh/sswfive/blog-pic@main/20230810/image.z212mi7o1og.png#id=hqu47&originHeight=650&originWidth=1509&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

### 机器学习通用流程
![](https://cdn.staticaly.com/gh/sswfive/blog-pic@main/20230810/image.oqfcxupoqs0.webp#id=WoL2w&originHeight=666&originWidth=1565&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
#### step1: 获取数据

- 获取数据的方式有：爬虫、物联网设备、数据库、文件等等
#### step2:数据基本处理

- 对数据进行缺失值、去除异常值等处理
#### step3:特征工程
> 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。

- 概念：
   - 特征工程是指使用专业背景知识和技巧处理数据，使得特征能在几区学习算法上发挥更好的作用的过程
   - 特征工程的意义：会直接影响技机器学习的效果
- 方法：
   - **特征提取**：将任意数据（文本或图像）转换为可用机器学习的数字特征。
   - **特征预处理**：通过一些转换函数将特征数据转换成更适合算法模型的特征数据过程。
   - **特征降维**：指在某些限定条件下，降低随机变量(特征)的个数，得到一组“不相关”主变量额过程
#### step4:机器学习（模型训练）

- 选择合适的算法对模型进行训练
#### step5:模型评估

- 概念：
   - 对训练好的模型进行评估
- 方法：
   - 分类模型评估
      - 准确率
      - 精确率
      - 召回率
      - Fl-score
      - AUC指标
   - 回归模型评估
      - 均方根误差
      - 相对平方误差
      - 平均绝对误差
      - 相对绝对误差
      - 决定系数
### 机器学习的完整流程
#### step1:抽象成数据问题
明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。这里的抽象成数学问题，指的明确我们可以获得什么样的数据，抽象出的问题，是一个分类还是回归或者是聚类的问题。
#### step2:获取数据
数据要有代表性，否则必然会过拟合。对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数量级的差距。
而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。
#### step3:特征预处理与特征选择
良好的数据要能够提取出良好的特征才能真正发挥作用。
特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。
归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。
筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。
#### step4:训练模型与调优
直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。
#### step5:模型诊断
如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。
过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。
误差分析 也是机器学习至关重要的步骤。通过观察误差样本全面分析产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……
诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。
#### step6:模型融合
一般来说，模型融合后都能使得效果有一定提升。而且效果很好。
工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。
#### step7:上线运行
这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。
这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。
